import os
import json
import time
from PyPDF2 import PdfReader
from fpdf import FPDF
from pathlib import Path

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_experimental.graph_transformers import LLMGraphTransformer

# ---------- Text Extraction ----------
def extract_text_from_pdf(pdf_path):
    try:
        with open(pdf_path, "rb") as f:
            reader = PdfReader(f)
            text = ""
            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + " "
                except Exception as e:
                    print(f"Error reading page {i+1} in {pdf_path}: {e}")
            return text.strip()
    except Exception as e:
        print(f"Error processing {pdf_path}: {e}")
        return ""

def extract_text_from_txt(txt_path):
    try:
        with open(txt_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"Error reading {txt_path}: {e}")
        return ""

def load_all_papers(directory):
    papers = {}
    for filename in os.listdir(directory):
        path = os.path.join(directory, filename)
        if filename.endswith(".pdf"):
            text = extract_text_from_pdf(path)
            if text:
                papers[filename] = text
        elif filename.endswith(".txt"):
            text = extract_text_from_txt(path)
            if text:
                papers[filename] = text
    print(f"Successfully loaded {len(papers)} papers")
    return papers

# ---------- Chunk-to-PDF Writer ----------
def save_chunks_to_pdf(chunks, paper_name, output_dir):
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)

    # Set font path to your downloaded font
    font_path = "/Users/alainasrivastav/Downloads/dmbi_proj/fonts/DejaVuSans.ttf"
    if not Path(font_path).exists():
        raise FileNotFoundError("DejaVuSans.ttf not found. Download it and place it in the fonts folder.")

    pdf.add_font("DejaVu", "", font_path, uni=True)
    pdf.set_font("DejaVu", "", 11)

    for i, chunk in enumerate(chunks):
        pdf.add_page()
        pdf.multi_cell(0, 10, f"Chunk {i+1}\n\n{chunk}")

    pdf_output_path = os.path.join(output_dir, f"{paper_name}_chunks.pdf")
    pdf.output(pdf_output_path)
    print(f"Saved chunk PDF: {pdf_output_path}")

# ---------- LLM Graph Extraction ----------
def extract_graph_from_chunks(chunks, llm):
    from langchain_core.documents import Document
    
    transformer = LLMGraphTransformer(llm=llm)
    all_entities = []
    all_relations = []

    for i, chunk in enumerate(chunks):
        print(f"Extracting from chunk {i+1}/{len(chunks)}")
        try:
            doc = Document(page_content=chunk)
            graph_documents = transformer.convert_to_graph_documents([doc])
            
            for graph_doc in graph_documents:
                all_entities.extend(graph_doc.nodes)
                all_relations.extend(graph_doc.relationships)
                
        except Exception as e:
            print(f"Error processing chunk {i+1}: {e}")
        time.sleep(1)

    return {
        "entities": [{"id": e.id, "type": e.type, "properties": e.properties} for e in all_entities],
        "relations": [{"source": r.source.id, "target": r.target.id, "type": r.type, "properties": r.properties} for r in all_relations]
    }

# ---------- Main Pipeline ----------
def main():
    input_dir = "/Users/alainasrivastav/Downloads/dmbi_proj/pdfs"
    output_dir = "/Users/alainasrivastav/Downloads/dmbi_proj/processed_graphs"
    os.makedirs(output_dir, exist_ok=True)

    papers = load_all_papers(input_dir)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    for paper_name, text in papers.items():
        base_name = os.path.splitext(paper_name)[0]
        kg_path = os.path.join(output_dir, f"{base_name}_kg.json")

        if os.path.exists(kg_path):
            print(f"Skipping {paper_name} (already processed)")
            continue

        chunks = splitter.split_text(text)
        print(f"\nProcessing {paper_name} ({len(chunks)} chunks)")

        # Save chunks to a PDF file for manual inspection
        save_chunks_to_pdf(chunks, base_name, output_dir)

        # Extract knowledge graph from chunks
        kg = extract_graph_from_chunks(chunks, llm)

        with open(kg_path, "w", encoding="utf-8") as f:
            json.dump(kg, f, indent=2)
        print(f"Saved KG JSON: {kg_path}")

if __name__ == "__main__":
    main()
