''' This file is create by Yushu Huang
for joint DMBI project of Cindy Lam, Alaina Srivastav and Yushu Huang.
Last revision 08/28/2025. 
Purpose of this file: to prompt for factor -> outcome summarization. 

Input Data: chunks as document.pkl. 
Chained prompt engineering'''

# %%
import os
import re
import json

from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import ChatOpenAI

from chunking import chunking


class prompt():
    def __init__(self, chunked_docs, outcome_definition):
        self.chunked_docs       = chunked_docs
        self.outcome_definition = outcome_definition

    def _message_find_factors_to_result(self, chunk):

        outcome_definition  = self.outcome_definition

        phenomenon_definition = "\n\n".join(
            f"- **{item['phenomenon']}**: {item['definition']}" for item in outcome_definition
        )
        response_format = "\n".join(
            f"The factors contributing to {item['phenomenon']} are: (list all factors contributing to {item['phenomenon']} only)"
            for item in outcome_definition
        )
        messages = [
            {"role": "system", 
            "content": (
                "You are an assistant skilled at extracting and summarizing key factors from text. "
                "While you do not possess specific domain expertise, you excel in information retrieval from provided content."
            )},
            {"role": "user", "content": (
                f"You are provided with context from a research study on digital mindfulness-based interventions. "
                f"Your task is to identify factors in the context that contributes to each of the following phenomena individually:\n\n"
                f"{phenomenon_definition}\n\n"
                f"Here is the context:\n{chunk}\n\n"
                "Your response should follow this format:\n"
                f"{response_format}\n\n"
                "For each phenomenon, identify all relevant factors from the context." 
                "For each factor, quote the specific part of the context that describe how this factor contribute to this phenomenon.  "
                # "Citations are provided within the context, enclosed in parentheses and formatted like 'Author et al., Year'. "
                "Citations are provided within the context, enclosed in brackets or parenthesis and formatted as an integer or integers separated by comma. "
                "If there are multiple citations, they are separated by a semicolon ';'. Please copy these citations directly into your response. "
                "If no factors are evident, respond with 'There are no related factors in this chunk.'"

            )}
        ]
        return messages
        
    def _message_find_factor_relation(self):
        ''' a prompt TBD. input: list of factors. output: find in context their relation, if the context mentioned. '''
        pass

    def generate_response(self, message_func, include_chunk=False):

        chunks  = self.chunked_docs
        results = []

        for i in range(len(chunks)):
            print(f'processing chunk {i}')
            chunk = chunks[i]
            if include_chunk == True:
                results.append(chunk.page_content)
            message  = message_func(chunk)
            response = client.chat.completions.create(model="gpt-4o",
            messages=message,
            max_tokens=1000,
            temperature=0)
            results.append(response.choices[0].message.content.strip())
        
        return results

    def write_summaries_to_txt(self, textlist_to_write, output_file):
        '''text_to_write: a list of texts. Each entry is a text generated by analyze_chunk'''
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for text in textlist_to_write:
                outfile.write(text + "\n\n")
        print("Analyze results saved to text file. ")

    # def generate_summary_files(self, output_file, include_chunk=False): 
    #     outcome_definition  = self.outcome_definition
    #     chunks              = self.chunked_docs

    #     phenomena_def = "\n\n".join(
    #         f"- **{item['phenomenon']}**: {item['definition']}" for item in outcome_definition
    #     )
    #     expected_format = "\n".join(
    #         f"The factors contributing to {item['phenomenon']} are: (list all factors contributing to {item['phenomenon']} only)"
    #         for item in outcome_definition
    #     )



    #     result = []
    #     # output_file = 'response.txt'
    #     for i in range(len(chunks)):
    #         print(f'processing chunk {i}')
    #         if include_chunk == True:
    #             result.append(chunks[i].page_content)
    #         result.append(self._find_factors_to_result(phenomena_def, chunks[i], response_format=expected_format))
    #     print(f"Start saving response")
    #     self.write_summaries_to_txt(result, output_file)
    #     print(f"End saving response")
    #     # TODO: Make it more robust by removing summary list, but saving them chunk by chunk. 

    #     print("Done with generating summary files")





if __name__ == "__main__":
    path = os.getcwd()
    chunk_subpath = 'cleaned_papers/tochunk/'
    chunk_folder = os.path.join(path, chunk_subpath)
    chunk = chunking(chunk_folder)
    documents = chunk.load_chunks()

    outcome_filename = 'outcome_definition.json'
    with open(os.path.join(path, outcome_filename)) as outcome_file:
        outcome_definition = json.load(outcome_file)

    generate_prompt = prompt(chunked_docs = documents, outcome_definition = outcome_definition)
    message_func = generate_prompt._message_find_factors_to_result
    results = generate_prompt.generate_response(message_func, include_chunk=True)
    generate_prompt.write_summaries_to_txt(results, 'response.txt')




