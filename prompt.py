''' This file is create by Yushu Huang
for joint DMBI project of Cindy Lam, Alaina Srivastav and Yushu Huang.
Last revision 08/28/2025. 
Purpose of this file: to prompt for factor -> outcome summarization. 

Input Data: chunks as document.pkl. 
Chained prompt engineering'''

# %%
import os
import re
import json

from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_openai import ChatOpenAI

from chunking import chunking


class prompt():
    def __init__(self, chunked_docs, outcome_definition):
        self.chunked_docs       = chunked_docs
        self.outcome_definition = outcome_definition

    def _find_factors_to_result(self, phenomenon_definition, chunk, response_format):
        messages = [
            {"role": "system", 
            "content": (
                "You are an assistant skilled at extracting and summarizing key factors from text. "
                "While you do not possess specific domain expertise, you excel in information retrieval from provided content."
            )},
            {"role": "user", "content": (
                f"You are provided with context from a research study on digital mindfulness-based interventions. "
                f"Your task is to identify factors in the context that contributes to each of the following phenomena individually:\n\n"
                f"{phenomenon_definition}\n\n"
                f"Here is the context:\n{chunk}\n\n"
                "Your response should follow this format:\n"
                f"{response_format}\n\n"
                "For each phenomenon, identify all relevant factors from the context." 
                "For each factor, quote the specific part of the context that describe how this factor contribute to this phenomenon.  "
                "Citations are provided within the context, enclosed in parentheses and formatted like 'Author et al., Year'. "
                "If there are multiple citations, they are separated by a semicolon ';'. Please copy these citations directly into your response. "
                "If no factors are evident, respond with 'There are no related factors in this chunk.'"

            )}
        ]
        response = client.chat.completions.create(model="gpt-4o",
        messages=messages,
        max_tokens=1000,
        temperature=0)
        return response.choices[0].message.content.strip()
    def _find_factor_relation(self):
        ''' a prompt TBD. input: list of factors. output: find in context their relation, if the context mentioned. '''
        pass

    def write_summaries_to_txt(self, textlist_to_write, output_file):
        '''text_to_write: a list of texts. Each entry is a text generated by analyze_chunk'''
        with open(output_file, 'w', encoding='utf-8') as outfile:
            for text in textlist_to_write:
                outfile.write(text + "\n\n")
        print("Analyze results saved to text file. ")

    def generate_summary_files(self): 
        outcome_definition  = self.outcome_definition
        chunks              = self.chunked_docs

        phenomena_def = "\n\n".join(
            f"- **{item['phenomenon']}**: {item['definition']}" for item in outcome_definition
        )
        expected_format = "\n".join(
            f"The factors contributing to {item['phenomenon']} are: (list all factors contributing to {item['phenomenon']} only)"
            for item in outcome_definition
        )



        result = []
        output_file = 'response.txt'
        for chunk in chunks:
            print(f'processing chunk {i}')
            result.append(self._find_factors_to_result(phenomena_def, chunk, response_format=expected_format))
        print(f"Start saving response")
        self.write_summaries_to_txt(result, output_file)
        print(f"End saving response")
        # TODO: Make it more robust by removing summary list, but saving them chunk by chunk. 

        print("Done with generating summary files")





if __name__ == "__main__":
    path = os.getcwd()
    chunk_subpath = 'chunks'
    chunk_folder = os.path.join(path, chunk_subpath)
    chunk = chunking(chunk_folder)
    documents = chunk.load_chunks()

    outcome_filename = 'outcome_definition.json'
    with open(os.path.join(path, outcome_filename)) as outcome_file:
        outcome_definition = json.load(outcome_file)

    generate_prompt = prompt(chunked_docs = documents, outcome_definition = outcome_definition)
    generate_prompt.generate_summary_files()



